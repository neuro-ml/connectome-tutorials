{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the transforms are time-consuming. For example, `Zoom`, from previous tutorials is implemented via linear interpolation, which is quite expensive, especially for objects of higher dimension, such as 3D images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching to RAM\n",
    "\n",
    "A popular way of dealing with this complexity is caching. We'll start by the simplest of them - caching to RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a dataset\n",
    "from layers02 import *\n",
    "from connectome import Chain\n",
    "\n",
    "source = HeLa(root='DIC-C2DH-HeLa')\n",
    "key = source.ids[0]\n",
    "\n",
    "dataset = Chain(\n",
    "    source, \n",
    "    Binarize(),\n",
    "    Zoom(factor=0.25),\n",
    "    Crop(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.4 ms ± 736 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x, y = dataset.image(key), dataset.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is is not much, but for larger images the time would be greater.\n",
    "\n",
    "Now, let's cache this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connectome import CacheToRam\n",
    "\n",
    "cached = dataset >> CacheToRam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first call will take around the same time, because the data need to be cached first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.8 ms, sys: 58.7 ms, total: 153 ms\n",
      "Wall time: 50.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but subsequent calls will be much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 5.45 ms, total: 5.45 ms\n",
      "Wall time: 1.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 ms ± 10.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we sped up our pipeline by a factor of 50. Now this is _fast_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Caching to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching to RAM does speed up our pipelines, but we still have a problem - the first call to `image` and `mask` are slow, because the computation need to happen in the first place. This means that each time you restart your script (or create the pipeline, for that matter) you'll have to recompute the cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "cached = dataset >> CacheToRam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.4 ms, sys: 52.1 ms, total: 133 ms\n",
      "Wall time: 57.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first call - slow\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.13 ms, sys: 1.08 ms, total: 5.21 ms\n",
      "Wall time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# second call - fast\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset again\n",
    "cached = dataset >> CacheToRam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 20.5 ms, total: 146 ms\n",
      "Wall time: 48.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# first call - slow again!\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we could make a persistent cache that keeps living between runs?\n",
    "\n",
    "Well, we can! This is when caching to disk comes into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connectome import CacheToDisk\n",
    "\n",
    "cached = dataset >> CacheToDisk.simple('image', 'mask', root='cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is happening here? We want to cache `image` and `mask` and will be storing the cache in the current directory, in the `cache` folder. You can change the path if you like.\n",
    "\n",
    "`CacheToDisk` is a highly customizable layer, however for this tutorial `simple` is a good starting point - it will choose adequate default parameters for you.\n",
    "\n",
    "The first run is slow as always:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 179 ms, sys: 135 ms, total: 314 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 ms, sys: 13.4 ms, total: 46.8 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next calls are faster. \n",
    "\n",
    "Now let's create the dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = dataset >> CacheToDisk.simple('image', 'mask', root='cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.4 ms, sys: 11.8 ms, total: 59.2 ms\n",
      "Wall time: 20.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even the first call is fast too! It's not as fast as caching to RAM, but we can combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = Chain(\n",
    "    dataset,\n",
    "    CacheToDisk.simple('image', 'mask', root='cache'),\n",
    "    CacheToRam(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 ms, sys: 0 ns, total: 18.6 ms\n",
      "Wall time: 19.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 ms, sys: 0 ns, total: 3.39 ms\n",
      "Wall time: 3.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x, y = cached.image(key), cached.mask(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the best of both worlds. How neat is that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Invalidation\n",
    "\n",
    "Now our cache is stored in the `cache` folder and it is loaded from disk when it's needed. There is a potential problem. What if we change the data preprocessing? Do we need to choose a new folder for the cache?\n",
    "\n",
    "Luckily, the answer is _no, we don't_. `connectome` is smart enough to figure out that the data has changed, and it will always keep the cache _consistent_ with your current data!\n",
    "\n",
    "Watch this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = Chain(\n",
    "    source, \n",
    "    Binarize(),\n",
    "    Zoom(factor=0.25),\n",
    "    Crop(),\n",
    ") >> CacheToDisk.simple('image', 'mask', root='cache')\n",
    "\n",
    "big = Chain(\n",
    "    source, \n",
    "    Binarize(),\n",
    "    Zoom(factor=0.5),\n",
    "    Crop(),\n",
    ") >> CacheToDisk.simple('image', 'mask', root='cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two datasets with different transformations: the first one downsamples the images by a factor of 4, the second one - by 2. \n",
    "\n",
    "Let's check the image's shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((122, 120), (244, 243))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill the cache\n",
    "small.image(key).shape, big.image(key).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((122, 120), (244, 243))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from cache\n",
    "small.image(key).shape, big.image(key).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **automatic cache invalidation** at work!\n",
    "\n",
    "That's all for caching. See you in next tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
